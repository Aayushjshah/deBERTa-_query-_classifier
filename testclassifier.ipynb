{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:21:05.706712Z",
     "iopub.status.busy": "2025-05-16T04:21:05.706385Z",
     "iopub.status.idle": "2025-05-16T04:21:05.712522Z",
     "shell.execute_reply": "2025-05-16T04:21:05.711568Z",
     "shell.execute_reply.started": "2025-05-16T04:21:05.706690Z"
    }
   },
   "source": [
    "# Runnable Notebook for Testing the trained model\n",
    "\n",
    "- Model hosted over kaggle : https://www.kaggle.com/models/shah2001aayush/deberta_classification\n",
    "- Dataset to generate Test data : https://www.kaggle.com/datasets/shah2001aayush/dataprocessed\n",
    "- Use GPU for generating LLM final query\n",
    "- **Important - Change HuggingFace Token below** - to access Mistral LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:18:21.819919Z",
     "iopub.status.busy": "2025-05-16T04:18:21.819297Z",
     "iopub.status.idle": "2025-05-16T04:18:21.824850Z",
     "shell.execute_reply": "2025-05-16T04:18:21.823802Z",
     "shell.execute_reply.started": "2025-05-16T04:18:21.819895Z"
    }
   },
   "source": [
    "Name  : **Aayush Shah**\n",
    "\n",
    "\n",
    "email : 2001aayushshah@gmail.com\n",
    "\n",
    "\n",
    "Contact : +91 8879090901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples and flow of user query  -> prediction -> task_query_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/PZtV4SXG\"><img src=\"https://i.ibb.co/0jQzZw4y/Calendar-Prediction-Example.png\" alt=\"Calendar-Prediction-Example\" border=\"0\"></a>\n",
    "<a href=\"https://ibb.co/99fQHt7w\"><img src=\"https://i.ibb.co/ccp4hLjb/Calendar-Prediction-Example2.png\" alt=\"Calendar-Prediction-Example2\" border=\"0\"></a>\n",
    "<a href=\"https://ibb.co/scwsWY1\"><img src=\"https://i.ibb.co/X6sFYR5/email-Example-Pred.png\" alt=\"email-Example-Pred\" border=\"0\"></a>\n",
    "\n",
    "### Executing generated Task Query on Gmail Directly to fetch the desired emails\n",
    "\n",
    "<a href=\"https://ibb.co/Fb51yXqB\"><img src=\"https://i.ibb.co/wNSqP7rK/gmail-Search-Result.png\" alt=\"gmail-Search-Result\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Classification with DeBERTa V3 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details\n",
    "* **MODEL:** DeBERTa-v3-base\n",
    "* **Architecture:** GPU P-100 (Kaggle)\n",
    "* **Framework:** PyTorch, Transformers (Hugging Face library)\n",
    "* **Transformer Model:** microsoft/deberta-v3-base\n",
    "* **Tokenizer:** microsoft/deberta-v3-base\n",
    "* **Loss Metric:** F1 score\n",
    "* **Inference Metrics:** Precision, F1 score, Recall, Accuracy\n",
    "* **Logging:** MLflow (Experiment Name: \"Query_Classification\")\n",
    "* **Device:** CUDA\n",
    "* **Post-Prediction Modification:** LLM - Mistral:7b\n",
    "* **Time Taken:** 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why DeBERTa-v3-base?\n",
    "\n",
    "* **DeBERTa V3 (by Microsoft):** The chosentransformer model is `microsoft/deberta-v3-base`.\n",
    "* **Improved Contextual Understanding:** It introduces **disentangled attention**, which allows the model to better understand the relationships between words and their positions in a sentence, leading to improved contextual understanding.\n",
    "* **Strong Classification Performance:** DeBERTa V3 is **ranked among the top** models on many classification benchmarks, indicating its effectiveness for this type of task.\n",
    "* **Tokenizer: DebertaV2Tokenizer:** The associated tokenizer is `DebertaV2Tokenizer`.\n",
    "* **Excellent Accuracy:** DeBERTa models often achieve **excellent accuracy**, frequently outperforming RoBERTa on classification tasks.\n",
    "* Latest in comparsion to other transformer models  : Released in 2021 . compared to Bert , Roberta in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:11:35.063279Z",
     "iopub.status.busy": "2025-05-16T04:11:35.063020Z",
     "iopub.status.idle": "2025-05-16T04:11:46.610971Z",
     "shell.execute_reply": "2025-05-16T04:11:46.609848Z",
     "shell.execute_reply.started": "2025-05-16T04:11:35.063258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#installation of required libraries\n",
    "!pip install -q transformers datasets scikit-learn mlflow sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:11:52.529237Z",
     "iopub.status.busy": "2025-05-16T04:11:52.528918Z",
     "iopub.status.idle": "2025-05-16T04:12:27.361583Z",
     "shell.execute_reply": "2025-05-16T04:12:27.360818Z",
     "shell.execute_reply.started": "2025-05-16T04:11:52.529205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 04:12:08.153896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747368728.384197      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747368728.457288      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:31.697521Z",
     "iopub.status.busy": "2025-05-16T04:12:31.696473Z",
     "iopub.status.idle": "2025-05-16T04:12:31.701028Z",
     "shell.execute_reply": "2025-05-16T04:12:31.700312Z",
     "shell.execute_reply.started": "2025-05-16T04:12:31.697498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LEN = 32\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:36.779787Z",
     "iopub.status.busy": "2025-05-16T04:12:36.779459Z",
     "iopub.status.idle": "2025-05-16T04:12:36.802995Z",
     "shell.execute_reply": "2025-05-16T04:12:36.802452Z",
     "shell.execute_reply.started": "2025-05-16T04:12:36.779764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/dataprocessed/data_preprocessed.csv\")  # <-- Change path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:38.692857Z",
     "iopub.status.busy": "2025-05-16T04:12:38.692339Z",
     "iopub.status.idle": "2025-05-16T04:12:38.698202Z",
     "shell.execute_reply": "2025-05-16T04:12:38.697634Z",
     "shell.execute_reply.started": "2025-05-16T04:12:38.692832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:40.920253Z",
     "iopub.status.busy": "2025-05-16T04:12:40.919548Z",
     "iopub.status.idle": "2025-05-16T04:12:42.710133Z",
     "shell.execute_reply": "2025-05-16T04:12:42.709435Z",
     "shell.execute_reply.started": "2025-05-16T04:12:40.920230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283d8f9ee0c8409a88598619e4bd8d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602d0283e7114df4bbb783468f8e03ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb41907e5c249b1a0c9b04d1894b4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load DeBERTa Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use below class to prepare data  for custom testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:46.539004Z",
     "iopub.status.busy": "2025-05-16T04:12:46.538751Z",
     "iopub.status.idle": "2025-05-16T04:12:46.544438Z",
     "shell.execute_reply": "2025-05-16T04:12:46.543615Z",
     "shell.execute_reply.started": "2025-05-16T04:12:46.538985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, queries, labels, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        query = str(self.queries[item])\n",
    "        label = int(self.labels[item])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            query,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:48.290703Z",
     "iopub.status.busy": "2025-05-16T04:12:48.290394Z",
     "iopub.status.idle": "2025-05-16T04:12:48.309694Z",
     "shell.execute_reply": "2025-05-16T04:12:48.308952Z",
     "shell.execute_reply.started": "2025-05-16T04:12:48.290677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['Query'], df['label'], test_size=0.3, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_dataset = QueryDataset(train_texts.values, train_labels.values, tokenizer, MAX_LEN)\n",
    "test_dataset = QueryDataset(temp_texts.values, temp_labels.values, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:53.103027Z",
     "iopub.status.busy": "2025-05-16T04:12:53.102349Z",
     "iopub.status.idle": "2025-05-16T04:12:53.106763Z",
     "shell.execute_reply": "2025-05-16T04:12:53.106159Z",
     "shell.execute_reply.started": "2025-05-16T04:12:53.103005Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:54.600304Z",
     "iopub.status.busy": "2025-05-16T04:12:54.599629Z",
     "iopub.status.idle": "2025-05-16T04:12:54.604297Z",
     "shell.execute_reply": "2025-05-16T04:12:54.603655Z",
     "shell.execute_reply.started": "2025-05-16T04:12:54.600277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:55.774717Z",
     "iopub.status.busy": "2025-05-16T04:12:55.774411Z",
     "iopub.status.idle": "2025-05-16T04:12:55.778508Z",
     "shell.execute_reply": "2025-05-16T04:12:55.777802Z",
     "shell.execute_reply.started": "2025-05-16T04:12:55.774696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:12:56.891740Z",
     "iopub.status.busy": "2025-05-16T04:12:56.891448Z",
     "iopub.status.idle": "2025-05-16T04:12:57.719890Z",
     "shell.execute_reply": "2025-05-16T04:12:57.719126Z",
     "shell.execute_reply.started": "2025-05-16T04:12:56.891719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# preprocessing function same as the one used while training the model\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "retain_words = {\n",
    "    \"what\", \"how\", \"when\", \"where\", \"who\", \"which\", \"whom\", \"whose\", \"why\",\n",
    "    \"can\", \"should\", \"would\", \"could\", \"do\", \"did\", \"does\", \"will\", \"may\",\n",
    "    \"show\", \"find\", \"search\", \"get\", \"have\"\n",
    "}\n",
    "\n",
    "# Base stopwords from NLTK\n",
    "default_stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Final custom stopword list\n",
    "custom_stop_words = default_stop_words - retain_words\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_query(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Spell correction using TextBlob\n",
    "    text = str(TextBlob(text).correct())\n",
    "\n",
    "    # 3. Remove punctuation except '@'\n",
    "    text = re.sub(r\"[^\\w\\s@.]\", \"\", text)\n",
    "\n",
    "    # 4. Remove stop words\n",
    "    words = text.split()\n",
    "    # words = [\n",
    "    #     # lemmatizer.lemmatize(word)\n",
    "    #     for word in words\n",
    "    #     if word not in custom_stop_words\n",
    "    # ]\n",
    "\n",
    "    filtered_words = [word for word in words if word not in custom_stop_words]\n",
    "\n",
    "    # 5. Join and strip\n",
    "    text = \" \".join(filtered_words).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Mistral LLM Model for executable query generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:13:00.325633Z",
     "iopub.status.busy": "2025-05-16T04:13:00.325127Z",
     "iopub.status.idle": "2025-05-16T04:13:57.578441Z",
     "shell.execute_reply": "2025-05-16T04:13:57.577637Z",
     "shell.execute_reply.started": "2025-05-16T04:13:00.325611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: CUDA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d8bb0e4e5c43c9a5b7b43669f3c878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273622ef413e4d8b966756b5073d59a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588f12f765ec400f9396597b156ca8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0925ef3a575845b68940e603e6dad55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a76cbc45424c3da9ae3550aa12b3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8231f780a97343d4b29d85ec5e4a4c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57b176686924142ae489375e19a0722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478e03ba92754da0bf07db6055d47e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82b3c1778074d8b83403a6217fa66d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b545c2e045dd4a7bbb58783559886172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06af1eb2de4f4630bc46c197acdccd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login() # Replace with your actual token\n",
    "\n",
    "# âš™ï¸ Step 3: Hardware detection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device.upper()}\")\n",
    "\n",
    "# âœ… Use fp16 for GPU, fallback to fp32 for CPU\n",
    "precision = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# ğŸ§  Step 4: Load mistral model\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=precision,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", max_new_tokens=256, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:14:04.362717Z",
     "iopub.status.busy": "2025-05-16T04:14:04.362416Z",
     "iopub.status.idle": "2025-05-16T04:14:04.367777Z",
     "shell.execute_reply": "2025-05-16T04:14:04.366949Z",
     "shell.execute_reply.started": "2025-05-16T04:14:04.362697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_email_search_query(email_text):\n",
    "    \"\"\"\n",
    "    Generates an advanced search query using an LLM.\n",
    "\n",
    "    Args:\n",
    "        email_text (str): The text of the email.\n",
    "        classification_label (str): The classification label (e.g., \"calendar\", \"email\").\n",
    "\n",
    "    Returns:\n",
    "        str: The generated search query.\n",
    "    \"\"\"\n",
    "    #  <----------------------- LLM Interaction --------------------------->\n",
    "    #  This is the core part where you'd use Mistral.\n",
    "    #  The prompt should guide Mistral to generate a good search query.\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping a user search their emails.  Generate advanced search queries for Gmail based on the user's request.  Use Gmail's search operators (from:, to:, subject:, has:, etc.) to make the search as precise as possible.  Here are some examples:\n",
    "\n",
    "Text Query: Find my email from John about the project.\n",
    "Gmail Advanced Search Query: from:john subject:project\n",
    "\n",
    "Text Query: Show me the email my boss sent last week.\n",
    "Gmail Advanced Search Query: from:boss@example.com after:2024/05/08 before:2024/05/15\n",
    "\n",
    "Text Query: I need the document attached to Mary's email.\n",
    "Gmail Advanced Search Query: from:mary has:attachment has:document\n",
    "\n",
    "Text Query: Search for the meeting agenda.\n",
    "Gmail Advanced Search Query: subject:meeting agenda\n",
    "\n",
    "Text Query: Find the email where I was copied.\n",
    "Gmail Advanced Search Query: cc:me@example.com OR bcc:me@example.com\n",
    "\n",
    "Text Query: Show me the email about the party from Susan before Friday.\n",
    "Gmail Advanced Search Query: from:susan subject:party before:2024/05/10\n",
    "\n",
    "Text Query: Find the email with the spreadsheet.\n",
    "Gmail Advanced Search Query: has:spreadsheet\n",
    "\n",
    "Text Query: Show me the email from the mailing list.\n",
    "Gmail Advanced Search Query: list:info@example.org\n",
    "\n",
    "Text Query: I'm looking for the email with the presentation.\n",
    "Gmail Advanced Search Query: has:presentation\n",
    "\n",
    "Text Query: Find the email about the \"urgent\" report.\n",
    "Gmail Advanced Search Query: subject:\"urgent report\"\n",
    "\n",
    "Text Query: Show me emails from March 6th, 2023 mentioning 'Google'\n",
    "Gmail Advanced Search Query: after:2025/05/12 Google\n",
    "\n",
    "Text Query: Show me emails from January 11th, 2021 with word 'DRDO' in it\n",
    "Gmail Advanced Search Query: after:2021/01/11 DRDO\n",
    "\n",
    "Based  on the above examples generate  Gmail Advanced Search Query for the below Text Query . Answer 1 short search query and nothing else.\n",
    "Text Query: {email_text}\n",
    "Gmail Advanced Search Query:\"\"\"\n",
    "\n",
    "    \n",
    "    response = generator(prompt,max_new_tokens=500, temperature=0.1)[0][\"generated_text\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:14:06.445576Z",
     "iopub.status.busy": "2025-05-16T04:14:06.445292Z",
     "iopub.status.idle": "2025-05-16T04:14:10.705221Z",
     "shell.execute_reply": "2025-05-16T04:14:10.704473Z",
     "shell.execute_reply.started": "2025-05-16T04:14:06.445555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_calendar_search_query(query):\n",
    "    \"\"\"\n",
    "    Generates an advanced search query using an LLM.\n",
    "\n",
    "    Args:\n",
    "        email_text (str): The text of the email.\n",
    "        classification_label (str): The classification label (e.g., \"calendar\", \"email\").\n",
    "\n",
    "    Returns:\n",
    "        str: The generated search query.\n",
    "    \"\"\"\n",
    "    #  <----------------------- LLM Interaction --------------------------->\n",
    "    #  This is the core part where you'd use Mistral.\n",
    "    #  The prompt should guide Mistral to generate a good search query.\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant designed to generate calendar search queries based on user requests.  Each search query should have two parts:\n",
    "\n",
    "* **Action:** The action to perform (e.g., \"search\", \"create\", \"reschedule\", \"cancel\").\n",
    "* **Search:** The details to use for the search (e.g., \"meeting with John\", \"appointment with Sarah\").\n",
    "\n",
    "Here are some examples of user requests and the corresponding search queries:\n",
    "\n",
    "Example 1:\n",
    "User Request: Find my meeting with John.\n",
    "Search Query: Action: search, Search: meeting with John\n",
    "\n",
    "Example 2:\n",
    "User Request: Show me my appointments for tomorrow.\n",
    "Search Query: Action: search, Search: appointments tomorrow\n",
    "\n",
    "Example 3:\n",
    "User Request: When is my next appointment with the doctor?\n",
    "Search Query: Action: search, Search: next doctor appointment\n",
    "\n",
    "Example 4:\n",
    "User Request: Reschedule my meeting with the team to Friday.\n",
    "Search Query: Action: reschedule, Search: team meeting Friday\n",
    "\n",
    "Example 5:\n",
    "User Request: Find the meeting about the project on June 10th.\n",
    "Search Query: Action: search, Search: meeting about the project June 10th\n",
    "\n",
    "Example 6:\n",
    "User Request: Show me all my meetings next week.\n",
    "Search Query: Action: search, Search: meetings next week\n",
    "\n",
    "Example 7:\n",
    "User Request: Cancel my appointment with Sarah.\n",
    "Search Query: Action: cancel, Search: appointment Sarah\n",
    "\n",
    "Example 8:\n",
    "User Request: Find the event scheduled by the \"XYZ project group\".\n",
    "Search Query: Action: search, Search: event scheduled by XYZ project group\n",
    "\n",
    "Example 9:\n",
    "User Request: Show me all my events in the month of December.\n",
    "Search Query: Action: search, Search: events December\n",
    "\n",
    "Example 10:\n",
    "User Request: Find the event called \"conference\" that occurs in the next 3 days\n",
    "Search Query: Action: search, Search: conference next 3 days\n",
    "\n",
    "Example 11:\n",
    "User Request: Create a meeting with John tomorrow at 2pm\n",
    "Search Query: Action: create, Search: meeting with John tomorrow at 2pm\n",
    "\n",
    "Example 12:\n",
    "User Request: Update my meeting with John to next Monday\n",
    "Search Query: Action: update, Search: meeting with John to next Monday\n",
    "\n",
    "Based  on the above examples generate Search Query for the below Text Query . Answer 1 short search query and nothing else.\n",
    "Text Query: {query}\n",
    "Calendar Search Query:\"\"\"\n",
    "    response = generator(prompt,max_new_tokens=500, temperature=0.1)[0][\"generated_text\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:14:13.296322Z",
     "iopub.status.busy": "2025-05-16T04:14:13.296035Z",
     "iopub.status.idle": "2025-05-16T04:14:13.300679Z",
     "shell.execute_reply": "2025-05-16T04:14:13.300084Z",
     "shell.execute_reply.started": "2025-05-16T04:14:13.296300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_after_last_search_query(text):\n",
    "    pattern = r\"Search Query:\"\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "\n",
    "    if matches:\n",
    "        last_match = matches[-1]\n",
    "        return text[last_match.end():].strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:29:51.509160Z",
     "iopub.status.busy": "2025-05-16T04:29:51.508879Z",
     "iopub.status.idle": "2025-05-16T04:29:51.519810Z",
     "shell.execute_reply": "2025-05-16T04:29:51.519034Z",
     "shell.execute_reply.started": "2025-05-16T04:29:51.509140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "def get_prediction(text, model, tokenizer, max_len, device='cuda',gen_task_query=False):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given text using the trained model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to predict the sentiment for.\n",
    "        model (torch.nn.Module): The trained PyTorch model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the model.\n",
    "        max_len (int): The maximum sequence length.\n",
    "        device (str, optional): The device to use ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted sentiment ('Positive' or 'Negative')\n",
    "              and its corresponding probability.\n",
    "    \"\"\"\n",
    "    user_query = text\n",
    "    text = preprocess_query(text)  # Apply the preprocessing here\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu()).numpy()  # Get probabilities, move to CPU, convert to numpy\n",
    "\n",
    "    label = np.argmax(probs, axis=-1)\n",
    "    task_query = \"\"\n",
    "    # gen_task_query = True\n",
    "    if(gen_task_query == True):\n",
    "        if label == 1:\n",
    "            task_query = generate_calendar_search_query(user_query)\n",
    "            task_query = extract_after_last_search_query(task_query)\n",
    "        else:\n",
    "            task_query = generate_email_search_query(user_query)\n",
    "            task_query = extract_after_last_search_query(task_query)\n",
    "    \n",
    "    if label == 1:\n",
    "        return {\n",
    "            'prediction': 'Calendar',\n",
    "            'probability': probs[1],\n",
    "            'task_query' : task_query\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'prediction': 'Email',\n",
    "            'probability': probs[0],\n",
    "            'task_query' : task_query\n",
    "        }\n",
    "\n",
    "def display_predictions(model, test_dataset, tokenizer, max_len, device='cuda',gen_task_query=False):\n",
    "    \"\"\"\n",
    "    Displays the predictions for the queries in the test set.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained PyTorch model.\n",
    "        test_dataset (torch.utils.data.Dataset): The test dataset.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the model.\n",
    "        max_len (int): The maximum sequence length.\n",
    "        device (str, optional): The device to use ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "    \"\"\"\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        \n",
    "        sample = test_dataset[i]\n",
    "        text = test_dataset.queries[i]  # Access the original text from the dataset\n",
    "        if(gen_task_query):\n",
    "            prediction = get_prediction(text, model, tokenizer, max_len, device,True)\n",
    "        else:\n",
    "            prediction = get_prediction(text, model, tokenizer, max_len, device)\n",
    "        ground_truth = test_dataset.labels[i]  # Access the ground truth label.\n",
    "        all_labels.append(ground_truth)\n",
    "\n",
    "        if prediction['prediction'] == 'Calendar':\n",
    "            all_preds.append(1)\n",
    "        else:\n",
    "            all_preds.append(0)\n",
    "        if i < 20:\n",
    "            print(f\"Query: {text}\")\n",
    "            print(f\"Predicted Sentiment: {prediction['prediction']}\")\n",
    "            print(f\"Probability: {prediction['probability']:.4f}\")  # Format probability\n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "            if gen_task_query:\n",
    "                print(f\"Ground Truth: {prediction['task_query']}\")\n",
    "            print(\"-\" * 20)\n",
    "    \n",
    "    # Calculate and print metrics after the loop\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {precision:.4f}\")\n",
    "    print(f\"Overall Recall: {recall:.4f}\")\n",
    "    print(f\"Overall F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading trained Model to generate prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:29:54.280064Z",
     "iopub.status.busy": "2025-05-16T04:29:54.279551Z",
     "iopub.status.idle": "2025-05-16T04:29:55.273813Z",
     "shell.execute_reply": "2025-05-16T04:29:55.273238Z",
     "shell.execute_reply.started": "2025-05-16T04:29:54.280043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/deberta_classification/pytorch/default/1/').to('cuda') # Or 'cpu'\n",
    "    # 2.  Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating prediction for test_data\n",
    "- ## We are not generating executable queries for test data . If you wish to do so use below function setting gen_task_queries=True\n",
    "-   Use - **display_predictions(model, test_dataset, tokenizer, MAX_LEN,'cuda',True)**\n",
    "- We are printing first 20 predictions of test_data for display\n",
    "- Below entire results of test data with confusion matrix is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:31:08.510677Z",
     "iopub.status.busy": "2025-05-16T04:31:08.510142Z",
     "iopub.status.idle": "2025-05-16T04:31:25.590477Z",
     "shell.execute_reply": "2025-05-16T04:31:25.589917Z",
     "shell.execute_reply.started": "2025-05-16T04:31:08.510653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: search meeting agenda\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9353\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: what appointments next tuesday\n",
      "Predicted Sentiment: Calendar\n",
      "Probability: 0.9167\n",
      "Ground Truth: 1\n",
      "--------------------\n",
      "Query: email where tenders email address domain common one like email yakov.\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9651\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: show followup responses sent\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9665\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: find email companies asking shop\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9697\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: hey assistant what coming deadline week\n",
      "Predicted Sentiment: Calendar\n",
      "Probability: 0.8534\n",
      "Ground Truth: 1\n",
      "--------------------\n",
      "Query: where conference call clients located friday 2 pm\n",
      "Predicted Sentiment: Calendar\n",
      "Probability: 0.9191\n",
      "Ground Truth: 1\n",
      "--------------------\n",
      "Query: what came weekend\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9110\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: get email received past quarter\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9691\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: can schedule meeting team london sunday 3 pm local time\n",
      "Predicted Sentiment: Calendar\n",
      "Probability: 0.9249\n",
      "Ground Truth: 1\n",
      "--------------------\n",
      "Query: fetch email first week june\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9682\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: hey assistant do have office events schedule next week\n",
      "Predicted Sentiment: Calendar\n",
      "Probability: 0.9095\n",
      "Ground Truth: 1\n",
      "--------------------\n",
      "Query: email containing word urgent\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9683\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: search email specific message id.\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9709\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: find email sent specific date past monday.\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9697\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: show email title team meeting\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9665\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: email ive categories specific way.\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9687\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: email flagged have specific importance level.\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9675\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: show email received first monday month\n",
      "Predicted Sentiment: Email\n",
      "Probability: 0.9700\n",
      "Ground Truth: 0\n",
      "--------------------\n",
      "Query: march 2025 1000 seminar hall conflict\n",
      "Predicted Sentiment: Calendar\n",
      "Probability: 0.9235\n",
      "Ground Truth: 1\n",
      "--------------------\n",
      "Overall Accuracy: 1.0000\n",
      "Overall Precision: 1.0000\n",
      "Overall Recall: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[158   0]\n",
      " [  0 125]]\n"
     ]
    }
   ],
   "source": [
    "display_predictions(model, test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are examples of generating individual predictions with ot without final executable query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we are generating task_query via Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:52:35.444466Z",
     "iopub.status.busy": "2025-05-16T03:52:35.443570Z",
     "iopub.status.idle": "2025-05-16T03:52:38.135045Z",
     "shell.execute_reply": "2025-05-16T03:52:38.134417Z",
     "shell.execute_reply.started": "2025-05-16T03:52:35.444443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'Calendar', 'probability': 0.9089517, 'task_query': 'Action: search, Search: upcoming conferences or offsite events'}\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(\"What are my upcoming conferences or offsite events?\", model, tokenizer, MAX_LEN,'cuda',True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:50:39.454339Z",
     "iopub.status.busy": "2025-05-16T04:50:39.453740Z",
     "iopub.status.idle": "2025-05-16T04:50:41.377683Z",
     "shell.execute_reply": "2025-05-16T04:50:41.376986Z",
     "shell.execute_reply.started": "2025-05-16T04:50:39.454317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'Calendar', 'probability': 0.91840917, 'task_query': 'Action: cancel, Search: birthday party schedule tomorrow'}\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(\"cancel birthday party schedule tomorrow\", model, tokenizer, MAX_LEN,'cuda',True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we are **not** generating task_query setting **gen_task_query = False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:52:41.601509Z",
     "iopub.status.busy": "2025-05-16T03:52:41.601201Z",
     "iopub.status.idle": "2025-05-16T03:52:41.957572Z",
     "shell.execute_reply": "2025-05-16T03:52:41.956972Z",
     "shell.execute_reply.started": "2025-05-16T03:52:41.601487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'Calendar', 'probability': 0.9089517, 'task_query': ''}\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(\"What are my upcoming conferences or offsite events?\", model, tokenizer, MAX_LEN,'cuda',False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we are generating task_query setting **gen_task_query = True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T04:15:48.237533Z",
     "iopub.status.busy": "2025-05-16T04:15:48.236882Z",
     "iopub.status.idle": "2025-05-16T04:15:50.541574Z",
     "shell.execute_reply": "2025-05-16T04:15:50.540991Z",
     "shell.execute_reply.started": "2025-05-16T04:15:48.237511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'Email', 'probability': 0.9643836, 'task_query': 'after:2025/05/08 juspay'}\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(\"Show me emails from May 8th, 2025 mentioning 'juspay'\", model, tokenizer, MAX_LEN,'cuda',True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further , I have created a StreamLit app for interacting with the model\n",
    "\n",
    "<a href=\"https://ibb.co/0p6HfHy3\"><img src=\"https://i.ibb.co/ymZH8HFJ/image.png\" alt=\"image\" border=\"0\"></a>\n",
    "\n",
    "### LLM Model not enabled in streamlit app for now ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I am executing the generated task_query in gmail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/Fb51yXqB\"><img src=\"https://i.ibb.co/wNSqP7rK/gmail-Search-Result.png\" alt=\"gmail-Search-Result\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1429803418.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    <a href=\"https://ibb.co/0p6HfHy3\"><img src=\"https://i.ibb.co/ymZH8HFJ/image.png\" alt=\"image\" border=\"0\"></a>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Further , I have created a StreamLit app for interacting with the model\n",
    "\n",
    "<a href=\"https://ibb.co/0p6HfHy3\"><img src=\"https://i.ibb.co/ymZH8HFJ/image.png\" alt=\"image\" border=\"0\"></a>\n",
    "\n",
    "### LLM Model not enabled in streamlit app for now ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12313350,
     "datasetId": 7425419,
     "sourceId": 11821219,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12324846,
     "modelInstanceId": 325007,
     "sourceId": 395848,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai_age2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
